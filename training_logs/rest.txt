(parkinson) xxx@xxx:~/xxx/dream_parkinsons/py/dream_parkinsons/appsâŸ« python train_dnn.py --dataset="/home/xxx/xxx/" --output_directory="/home/xxx/xxx/" --fraction_of_data_set=0.2 --num_units=32 --n_jobs=2 --num_epochs=100 --batch_size=8 --signal=rest
Using TensorFlow backend.
INFO: Built generators with 23521 training samples and 10080 validation samples. We are using 4704 / 23521 for training and 10080 / 10080 for validation.
INFO: Started training feature extraction.
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to
====================================================================================================
input_1 (InputLayer)             (None, None, 13)      0
____________________________________________________________________________________________________
masking_1 (Masking)              (None, None, 13)      0           input_1[0][0]
____________________________________________________________________________________________________
lambda_1 (Lambda)                (None, 1, None, 13)   0           masking_1[0][0]
____________________________________________________________________________________________________
conv2d_2 (Conv2D)                (None, 32, None, 13)  320         lambda_1[0][0]
____________________________________________________________________________________________________
activation_1 (Activation)        (None, 32, None, 13)  0           conv2d_2[0][0]
____________________________________________________________________________________________________
average_pooling2d_1 (AveragePool (None, 1, None, 13)   0           lambda_1[0][0]
____________________________________________________________________________________________________
conv2d_3 (Conv2D)                (None, 32, None, 13)  9248        activation_1[0][0]
____________________________________________________________________________________________________
conv2d_1 (Conv2D)                (None, 32, None, 13)  64          average_pooling2d_1[0][0]
____________________________________________________________________________________________________
average_pooling2d_2 (AveragePool (None, 32, None, 13)  0           conv2d_3[0][0]
____________________________________________________________________________________________________
add_1 (Add)                      (None, 32, None, 13)  0           conv2d_1[0][0]
                                                                   average_pooling2d_2[0][0]
____________________________________________________________________________________________________
batch_normalization_1 (BatchNorm (None, 32, None, 13)  128         add_1[0][0]
____________________________________________________________________________________________________
activation_2 (Activation)        (None, 32, None, 13)  0           batch_normalization_1[0][0]
____________________________________________________________________________________________________
conv2d_4 (Conv2D)                (None, 32, None, 13)  9248        activation_2[0][0]
____________________________________________________________________________________________________
batch_normalization_2 (BatchNorm (None, 32, None, 13)  128         conv2d_4[0][0]
____________________________________________________________________________________________________
activation_3 (Activation)        (None, 32, None, 13)  0           batch_normalization_2[0][0]
____________________________________________________________________________________________________
conv2d_6 (Conv2D)                (None, 32, None, 13)  1056        add_1[0][0]
____________________________________________________________________________________________________
conv2d_5 (Conv2D)                (None, 32, None, 13)  9248        activation_3[0][0]
____________________________________________________________________________________________________
average_pooling2d_4 (AveragePool (None, 32, None, 13)  0           conv2d_6[0][0]
____________________________________________________________________________________________________
average_pooling2d_3 (AveragePool (None, 32, None, 13)  0           conv2d_5[0][0]
____________________________________________________________________________________________________
add_2 (Add)                      (None, 32, None, 13)  0           average_pooling2d_4[0][0]
                                                                   average_pooling2d_3[0][0]
____________________________________________________________________________________________________
batch_normalization_3 (BatchNorm (None, 32, None, 13)  128         add_2[0][0]
____________________________________________________________________________________________________
activation_4 (Activation)        (None, 32, None, 13)  0           batch_normalization_3[0][0]
____________________________________________________________________________________________________
conv2d_7 (Conv2D)                (None, 32, None, 13)  9248        activation_4[0][0]
____________________________________________________________________________________________________
batch_normalization_4 (BatchNorm (None, 32, None, 13)  128         conv2d_7[0][0]
____________________________________________________________________________________________________
activation_5 (Activation)        (None, 32, None, 13)  0           batch_normalization_4[0][0]
____________________________________________________________________________________________________
conv2d_9 (Conv2D)                (None, 32, None, 13)  1056        add_2[0][0]
____________________________________________________________________________________________________
conv2d_8 (Conv2D)                (None, 32, None, 13)  9248        activation_5[0][0]
____________________________________________________________________________________________________
average_pooling2d_6 (AveragePool (None, 32, None, 13)  0           conv2d_9[0][0]
____________________________________________________________________________________________________
average_pooling2d_5 (AveragePool (None, 32, None, 13)  0           conv2d_8[0][0]
____________________________________________________________________________________________________
add_3 (Add)                      (None, 32, None, 13)  0           average_pooling2d_6[0][0]
                                                                   average_pooling2d_5[0][0]
____________________________________________________________________________________________________
batch_normalization_5 (BatchNorm (None, 32, None, 13)  128         add_3[0][0]
____________________________________________________________________________________________________
activation_6 (Activation)        (None, 32, None, 13)  0           batch_normalization_5[0][0]
____________________________________________________________________________________________________
conv2d_10 (Conv2D)               (None, 32, None, 13)  9248        activation_6[0][0]
____________________________________________________________________________________________________
batch_normalization_6 (BatchNorm (None, 32, None, 13)  128         conv2d_10[0][0]
____________________________________________________________________________________________________
activation_7 (Activation)        (None, 32, None, 13)  0           batch_normalization_6[0][0]
____________________________________________________________________________________________________
conv2d_12 (Conv2D)               (None, 32, None, 13)  1056        add_3[0][0]
____________________________________________________________________________________________________
conv2d_11 (Conv2D)               (None, 32, None, 13)  9248        activation_7[0][0]
____________________________________________________________________________________________________
average_pooling2d_8 (AveragePool (None, 32, None, 13)  0           conv2d_12[0][0]
____________________________________________________________________________________________________
average_pooling2d_7 (AveragePool (None, 32, None, 13)  0           conv2d_11[0][0]
____________________________________________________________________________________________________
add_4 (Add)                      (None, 32, None, 13)  0           average_pooling2d_8[0][0]
                                                                   average_pooling2d_7[0][0]
____________________________________________________________________________________________________
batch_normalization_7 (BatchNorm (None, 32, None, 13)  128         add_4[0][0]
____________________________________________________________________________________________________
activation_8 (Activation)        (None, 32, None, 13)  0           batch_normalization_7[0][0]
____________________________________________________________________________________________________
conv2d_13 (Conv2D)               (None, 32, None, 13)  9248        activation_8[0][0]
____________________________________________________________________________________________________
batch_normalization_8 (BatchNorm (None, 32, None, 13)  128         conv2d_13[0][0]
____________________________________________________________________________________________________
activation_9 (Activation)        (None, 32, None, 13)  0           batch_normalization_8[0][0]
____________________________________________________________________________________________________
conv2d_15 (Conv2D)               (None, 32, None, 13)  1056        add_4[0][0]
____________________________________________________________________________________________________
conv2d_14 (Conv2D)               (None, 32, None, 13)  9248        activation_9[0][0]
____________________________________________________________________________________________________
average_pooling2d_10 (AveragePoo (None, 32, None, 13)  0           conv2d_15[0][0]
____________________________________________________________________________________________________
average_pooling2d_9 (AveragePool (None, 32, None, 13)  0           conv2d_14[0][0]
____________________________________________________________________________________________________
add_5 (Add)                      (None, 32, None, 13)  0           average_pooling2d_10[0][0]
                                                                   average_pooling2d_9[0][0]
____________________________________________________________________________________________________
batch_normalization_9 (BatchNorm (None, 32, None, 13)  128         add_5[0][0]
____________________________________________________________________________________________________
activation_10 (Activation)       (None, 32, None, 13)  0           batch_normalization_9[0][0]
____________________________________________________________________________________________________
conv2d_16 (Conv2D)               (None, 32, None, 13)  9248        activation_10[0][0]
____________________________________________________________________________________________________
batch_normalization_10 (BatchNor (None, 32, None, 13)  128         conv2d_16[0][0]
____________________________________________________________________________________________________
activation_11 (Activation)       (None, 32, None, 13)  0           batch_normalization_10[0][0]
____________________________________________________________________________________________________
conv2d_18 (Conv2D)               (None, 32, None, 13)  1056        add_5[0][0]
____________________________________________________________________________________________________
conv2d_17 (Conv2D)               (None, 32, None, 13)  9248        activation_11[0][0]
____________________________________________________________________________________________________
average_pooling2d_12 (AveragePoo (None, 32, None, 13)  0           conv2d_18[0][0]
____________________________________________________________________________________________________
average_pooling2d_11 (AveragePoo (None, 32, None, 13)  0           conv2d_17[0][0]
____________________________________________________________________________________________________
add_6 (Add)                      (None, 32, None, 13)  0           average_pooling2d_12[0][0]
                                                                   average_pooling2d_11[0][0]
____________________________________________________________________________________________________
batch_normalization_11 (BatchNor (None, 32, None, 13)  128         add_6[0][0]
____________________________________________________________________________________________________
activation_12 (Activation)       (None, 32, None, 13)  0           batch_normalization_11[0][0]
____________________________________________________________________________________________________
conv2d_19 (Conv2D)               (None, 1, None, 13)   289         activation_12[0][0]
____________________________________________________________________________________________________
lambda_2 (Lambda)                (None, None, 13)      0           conv2d_19[0][0]
____________________________________________________________________________________________________
bidirectional_1 (Bidirectional)  (None, None, 64)      11776       lambda_2[0][0]
____________________________________________________________________________________________________
soft_attention_1 (SoftAttention) (None, 64)            4224        bidirectional_1[0][0]
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 32)            2080        soft_attention_1[0][0]
____________________________________________________________________________________________________
batch_normalization_12 (BatchNor (None, 32)            128         dense_1[0][0]
____________________________________________________________________________________________________
activation_13 (Activation)       (None, 32)            0           batch_normalization_12[0][0]
____________________________________________________________________________________________________
dropout_1 (Dropout)              (None, 32)            0           activation_13[0][0]
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 32)            1056        dropout_1[0][0]
____________________________________________________________________________________________________
batch_normalization_13 (BatchNor (None, 32)            128         dense_2[0][0]
____________________________________________________________________________________________________
activation_14 (Activation)       (None, 32)            0           batch_normalization_13[0][0]
____________________________________________________________________________________________________
input_2 (InputLayer)             (None, 1)             0
____________________________________________________________________________________________________
input_3 (InputLayer)             (None, 1)             0
____________________________________________________________________________________________________
dropout_2 (Dropout)              (None, 32)            0           activation_14[0][0]
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 34)            0           input_2[0][0]
                                                                   input_3[0][0]
                                                                   dropout_2[0][0]
____________________________________________________________________________________________________
discriminator_output (Dense)     (None, 1)             35          concatenate_1[0][0]
====================================================================================================
Total params: 128,516
Trainable params: 127,684
Non-trainable params: 832
____________________________________________________________________________________________________
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to
====================================================================================================
input_1 (InputLayer)             (None, None, 13)      0
____________________________________________________________________________________________________
masking_1 (Masking)              (None, None, 13)      0           input_1[0][0]
____________________________________________________________________________________________________
lambda_1 (Lambda)                (None, 1, None, 13)   0           masking_1[0][0]
____________________________________________________________________________________________________
conv2d_2 (Conv2D)                (None, 32, None, 13)  320         lambda_1[0][0]
____________________________________________________________________________________________________
activation_1 (Activation)        (None, 32, None, 13)  0           conv2d_2[0][0]
____________________________________________________________________________________________________
average_pooling2d_1 (AveragePool (None, 1, None, 13)   0           lambda_1[0][0]
____________________________________________________________________________________________________
conv2d_3 (Conv2D)                (None, 32, None, 13)  9248        activation_1[0][0]
____________________________________________________________________________________________________
conv2d_1 (Conv2D)                (None, 32, None, 13)  64          average_pooling2d_1[0][0]
____________________________________________________________________________________________________
average_pooling2d_2 (AveragePool (None, 32, None, 13)  0           conv2d_3[0][0]
____________________________________________________________________________________________________
add_1 (Add)                      (None, 32, None, 13)  0           conv2d_1[0][0]
                                                                   average_pooling2d_2[0][0]
____________________________________________________________________________________________________
batch_normalization_1 (BatchNorm (None, 32, None, 13)  128         add_1[0][0]
____________________________________________________________________________________________________
activation_2 (Activation)        (None, 32, None, 13)  0           batch_normalization_1[0][0]
____________________________________________________________________________________________________
conv2d_4 (Conv2D)                (None, 32, None, 13)  9248        activation_2[0][0]
____________________________________________________________________________________________________
batch_normalization_2 (BatchNorm (None, 32, None, 13)  128         conv2d_4[0][0]
____________________________________________________________________________________________________
activation_3 (Activation)        (None, 32, None, 13)  0           batch_normalization_2[0][0]
____________________________________________________________________________________________________
conv2d_6 (Conv2D)                (None, 32, None, 13)  1056        add_1[0][0]
____________________________________________________________________________________________________
conv2d_5 (Conv2D)                (None, 32, None, 13)  9248        activation_3[0][0]
____________________________________________________________________________________________________
average_pooling2d_4 (AveragePool (None, 32, None, 13)  0           conv2d_6[0][0]
____________________________________________________________________________________________________
average_pooling2d_3 (AveragePool (None, 32, None, 13)  0           conv2d_5[0][0]
____________________________________________________________________________________________________
add_2 (Add)                      (None, 32, None, 13)  0           average_pooling2d_4[0][0]
                                                                   average_pooling2d_3[0][0]
____________________________________________________________________________________________________
batch_normalization_3 (BatchNorm (None, 32, None, 13)  128         add_2[0][0]
____________________________________________________________________________________________________
activation_4 (Activation)        (None, 32, None, 13)  0           batch_normalization_3[0][0]
____________________________________________________________________________________________________
conv2d_7 (Conv2D)                (None, 32, None, 13)  9248        activation_4[0][0]
____________________________________________________________________________________________________
batch_normalization_4 (BatchNorm (None, 32, None, 13)  128         conv2d_7[0][0]
____________________________________________________________________________________________________
activation_5 (Activation)        (None, 32, None, 13)  0           batch_normalization_4[0][0]
____________________________________________________________________________________________________
conv2d_9 (Conv2D)                (None, 32, None, 13)  1056        add_2[0][0]
____________________________________________________________________________________________________
conv2d_8 (Conv2D)                (None, 32, None, 13)  9248        activation_5[0][0]
____________________________________________________________________________________________________
average_pooling2d_6 (AveragePool (None, 32, None, 13)  0           conv2d_9[0][0]
____________________________________________________________________________________________________
average_pooling2d_5 (AveragePool (None, 32, None, 13)  0           conv2d_8[0][0]
____________________________________________________________________________________________________
add_3 (Add)                      (None, 32, None, 13)  0           average_pooling2d_6[0][0]
                                                                   average_pooling2d_5[0][0]
____________________________________________________________________________________________________
batch_normalization_5 (BatchNorm (None, 32, None, 13)  128         add_3[0][0]
____________________________________________________________________________________________________
activation_6 (Activation)        (None, 32, None, 13)  0           batch_normalization_5[0][0]
____________________________________________________________________________________________________
conv2d_10 (Conv2D)               (None, 32, None, 13)  9248        activation_6[0][0]
____________________________________________________________________________________________________
batch_normalization_6 (BatchNorm (None, 32, None, 13)  128         conv2d_10[0][0]
____________________________________________________________________________________________________
activation_7 (Activation)        (None, 32, None, 13)  0           batch_normalization_6[0][0]
____________________________________________________________________________________________________
conv2d_12 (Conv2D)               (None, 32, None, 13)  1056        add_3[0][0]
____________________________________________________________________________________________________
conv2d_11 (Conv2D)               (None, 32, None, 13)  9248        activation_7[0][0]
____________________________________________________________________________________________________
average_pooling2d_8 (AveragePool (None, 32, None, 13)  0           conv2d_12[0][0]
____________________________________________________________________________________________________
average_pooling2d_7 (AveragePool (None, 32, None, 13)  0           conv2d_11[0][0]
____________________________________________________________________________________________________
add_4 (Add)                      (None, 32, None, 13)  0           average_pooling2d_8[0][0]
                                                                   average_pooling2d_7[0][0]
____________________________________________________________________________________________________
batch_normalization_7 (BatchNorm (None, 32, None, 13)  128         add_4[0][0]
____________________________________________________________________________________________________
activation_8 (Activation)        (None, 32, None, 13)  0           batch_normalization_7[0][0]
____________________________________________________________________________________________________
conv2d_13 (Conv2D)               (None, 32, None, 13)  9248        activation_8[0][0]
____________________________________________________________________________________________________
batch_normalization_8 (BatchNorm (None, 32, None, 13)  128         conv2d_13[0][0]
____________________________________________________________________________________________________
activation_9 (Activation)        (None, 32, None, 13)  0           batch_normalization_8[0][0]
____________________________________________________________________________________________________
conv2d_15 (Conv2D)               (None, 32, None, 13)  1056        add_4[0][0]
____________________________________________________________________________________________________
conv2d_14 (Conv2D)               (None, 32, None, 13)  9248        activation_9[0][0]
____________________________________________________________________________________________________
average_pooling2d_10 (AveragePoo (None, 32, None, 13)  0           conv2d_15[0][0]
____________________________________________________________________________________________________
average_pooling2d_9 (AveragePool (None, 32, None, 13)  0           conv2d_14[0][0]
____________________________________________________________________________________________________
add_5 (Add)                      (None, 32, None, 13)  0           average_pooling2d_10[0][0]
                                                                   average_pooling2d_9[0][0]
____________________________________________________________________________________________________
batch_normalization_9 (BatchNorm (None, 32, None, 13)  128         add_5[0][0]
____________________________________________________________________________________________________
activation_10 (Activation)       (None, 32, None, 13)  0           batch_normalization_9[0][0]
____________________________________________________________________________________________________
conv2d_16 (Conv2D)               (None, 32, None, 13)  9248        activation_10[0][0]
____________________________________________________________________________________________________
batch_normalization_10 (BatchNor (None, 32, None, 13)  128         conv2d_16[0][0]
____________________________________________________________________________________________________
activation_11 (Activation)       (None, 32, None, 13)  0           batch_normalization_10[0][0]
____________________________________________________________________________________________________
conv2d_18 (Conv2D)               (None, 32, None, 13)  1056        add_5[0][0]
____________________________________________________________________________________________________
conv2d_17 (Conv2D)               (None, 32, None, 13)  9248        activation_11[0][0]
____________________________________________________________________________________________________
average_pooling2d_12 (AveragePoo (None, 32, None, 13)  0           conv2d_18[0][0]
____________________________________________________________________________________________________
average_pooling2d_11 (AveragePoo (None, 32, None, 13)  0           conv2d_17[0][0]
____________________________________________________________________________________________________
add_6 (Add)                      (None, 32, None, 13)  0           average_pooling2d_12[0][0]
                                                                   average_pooling2d_11[0][0]
____________________________________________________________________________________________________
batch_normalization_11 (BatchNor (None, 32, None, 13)  128         add_6[0][0]
____________________________________________________________________________________________________
activation_12 (Activation)       (None, 32, None, 13)  0           batch_normalization_11[0][0]
____________________________________________________________________________________________________
conv2d_19 (Conv2D)               (None, 1, None, 13)   289         activation_12[0][0]
____________________________________________________________________________________________________
lambda_2 (Lambda)                (None, None, 13)      0           conv2d_19[0][0]
____________________________________________________________________________________________________
bidirectional_1 (Bidirectional)  (None, None, 64)      11776       lambda_2[0][0]
____________________________________________________________________________________________________
soft_attention_1 (SoftAttention) (None, 64)            4224        bidirectional_1[0][0]
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 32)            2080        soft_attention_1[0][0]
____________________________________________________________________________________________________
batch_normalization_12 (BatchNor (None, 32)            128         dense_1[0][0]
____________________________________________________________________________________________________
activation_13 (Activation)       (None, 32)            0           batch_normalization_12[0][0]
____________________________________________________________________________________________________
dropout_1 (Dropout)              (None, 32)            0           activation_13[0][0]
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 32)            1056        dropout_1[0][0]
____________________________________________________________________________________________________
batch_normalization_13 (BatchNor (None, 32)            128         dense_2[0][0]
____________________________________________________________________________________________________
activation_14 (Activation)       (None, 32)            0           batch_normalization_13[0][0]
____________________________________________________________________________________________________
input_2 (InputLayer)             (None, 1)             0
____________________________________________________________________________________________________
input_3 (InputLayer)             (None, 1)             0
____________________________________________________________________________________________________
dropout_2 (Dropout)              (None, 32)            0           activation_14[0][0]
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 34)            0           input_2[0][0]
                                                                   input_3[0][0]
                                                                   dropout_2[0][0]
____________________________________________________________________________________________________
discriminator_output (Dense)     (None, 1)             35          concatenate_1[0][0]
====================================================================================================
Total params: 128,516
Trainable params: 127,684
Non-trainable params: 832
____________________________________________________________________________________________________
Epoch 1/100
2017-10-01 15:02:29.319067: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-10-01 15:02:29.319100: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-10-01 15:02:29.319111: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-10-01 15:02:29.319119: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed upCPU computations.
2017-10-01 15:02:29.319132: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-10-01 15:02:30.038568: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:
name: GeForce GTX 970
major: 5 minor: 2 memoryClockRate (GHz) 1.253
pciBusID 0000:01:00.0
Total memory: 3.94GiB
Free memory: 3.88GiB
2017-10-01 15:02:30.038591: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0
2017-10-01 15:02:30.038595: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y
2017-10-01 15:02:30.038600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 970, pci bus id: 0000:01:00.0)
588/588 [==============================] - 687s - loss: 0.7548 - acc: 0.5938 - val_loss: 0.5894 - val_acc: 0.6991
Epoch 2/100
588/588 [==============================] - 680s - loss: 0.6384 - acc: 0.6722 - val_loss: 0.5782 - val_acc: 0.6994
Epoch 3/100
588/588 [==============================] - 681s - loss: 0.5942 - acc: 0.7047 - val_loss: 0.5362 - val_acc: 0.7024
Epoch 4/100
588/588 [==============================] - 681s - loss: 0.5865 - acc: 0.7056 - val_loss: 0.5388 - val_acc: 0.6969
Epoch 5/100
588/588 [==============================] - 681s - loss: 0.5525 - acc: 0.7160 - val_loss: 0.5259 - val_acc: 0.6983
Epoch 6/100
588/588 [==============================] - 681s - loss: 0.5541 - acc: 0.7115 - val_loss: 0.5022 - val_acc: 0.7430
Epoch 7/100
588/588 [==============================] - 682s - loss: 0.5429 - acc: 0.7190 - val_loss: 0.5112 - val_acc: 0.6949
Epoch 8/100
588/588 [==============================] - 680s - loss: 0.5416 - acc: 0.7215 - val_loss: 0.4812 - val_acc: 0.7319
Epoch 9/100
588/588 [==============================] - 681s - loss: 0.5389 - acc: 0.7264 - val_loss: 0.4822 - val_acc: 0.7258
Epoch 10/100
588/588 [==============================] - 681s - loss: 0.5235 - acc: 0.7428 - val_loss: 0.4786 - val_acc: 0.7580
Epoch 11/100
588/588 [==============================] - 682s - loss: 0.5243 - acc: 0.7392 - val_loss: 0.4677 - val_acc: 0.7561
Epoch 12/100
588/588 [==============================] - 680s - loss: 0.5125 - acc: 0.7491 - val_loss: 0.4785 - val_acc: 0.7616
Epoch 13/100
587/588 [============================>.] - ETA: 0s - loss: 0.5199 - acc: 0.7468
588/588 [==============================] - 681s - loss: 0.5200 - acc: 0.7466 - val_loss: 0.4732 - val_acc: 0.7267
Epoch 14/100
588/588 [==============================] - 680s - loss: 0.5291 - acc: 0.7440 - val_loss: 0.4776 - val_acc: 0.7301
Epoch 15/100
588/588 [==============================] - 681s - loss: 0.5125 - acc: 0.7560 - val_loss: 0.4714 - val_acc: 0.7313
Epoch 16/100
588/588 [==============================] - 681s - loss: 0.5042 - acc: 0.7679 - val_loss: 0.4759 - val_acc: 0.7219
Epoch 17/100
588/588 [==============================] - 680s - loss: 0.5090 - acc: 0.7666 - val_loss: 0.4510 - val_acc: 0.7940
Epoch 18/100
588/588 [==============================] - 679s - loss: 0.5105 - acc: 0.7740 - val_loss: 0.4521 - val_acc: 0.7948
Epoch 19/100
588/588 [==============================] - 681s - loss: 0.4955 - acc: 0.7874 - val_loss: 0.4520 - val_acc: 0.7794
Epoch 20/100
588/588 [==============================] - 681s - loss: 0.4989 - acc: 0.7838 - val_loss: 0.4451 - val_acc: 0.7889
Epoch 21/100
588/588 [==============================] - 679s - loss: 0.4917 - acc: 0.7898 - val_loss: 0.4515 - val_acc: 0.7746
Epoch 22/100
588/588 [==============================] - 681s - loss: 0.4915 - acc: 0.7885 - val_loss: 0.4425 - val_acc: 0.7926
Epoch 23/100
588/588 [==============================] - 679s - loss: 0.4828 - acc: 0.7927 - val_loss: 0.4414 - val_acc: 0.7943
Epoch 24/100
588/588 [==============================] - 680s - loss: 0.4781 - acc: 0.8004 - val_loss: 0.4300 - val_acc: 0.8002
Epoch 25/100
588/588 [==============================] - 680s - loss: 0.4864 - acc: 0.7972 - val_loss: 0.4296 - val_acc: 0.8023
Epoch 26/100
588/588 [==============================] - 682s - loss: 0.4747 - acc: 0.8040 - val_loss: 0.4263 - val_acc: 0.8143
Epoch 27/100
588/588 [==============================] - 681s - loss: 0.4773 - acc: 0.7983 - val_loss: 0.4280 - val_acc: 0.8084
Epoch 28/100
588/588 [==============================] - 680s - loss: 0.4753 - acc: 0.8019 - val_loss: 0.4212 - val_acc: 0.8115
Epoch 29/100
588/588 [==============================] - 681s - loss: 0.4646 - acc: 0.8108 - val_loss: 0.4212 - val_acc: 0.8085
Epoch 30/100
588/588 [==============================] - 680s - loss: 0.4747 - acc: 0.8063 - val_loss: 0.4279 - val_acc: 0.8145
Epoch 31/100
588/588 [==============================] - 680s - loss: 0.4760 - acc: 0.8078 - val_loss: 0.4161 - val_acc: 0.8248
Epoch 32/100
588/588 [==============================] - 681s - loss: 0.4683 - acc: 0.8070 - val_loss: 0.4175 - val_acc: 0.8187
Epoch 33/100
588/588 [==============================] - 681s - loss: 0.4592 - acc: 0.8155 - val_loss: 0.4120 - val_acc: 0.8179
Epoch 34/100
588/588 [==============================] - 680s - loss: 0.4647 - acc: 0.8180 - val_loss: 0.4207 - val_acc: 0.8263
Epoch 35/100
588/588 [==============================] - 682s - loss: 0.4608 - acc: 0.8140 - val_loss: 0.4179 - val_acc: 0.8193
Epoch 36/100
588/588 [==============================] - 681s - loss: 0.4620 - acc: 0.8195 - val_loss: 0.4130 - val_acc: 0.8237
Epoch 37/100
588/588 [==============================] - 681s - loss: 0.4588 - acc: 0.8157 - val_loss: 0.4087 - val_acc: 0.8314
Epoch 38/100
588/588 [==============================] - 680s - loss: 0.4636 - acc: 0.8197 - val_loss: 0.4141 - val_acc: 0.8238
Epoch 39/100
588/588 [==============================] - 679s - loss: 0.4671 - acc: 0.8204 - val_loss: 0.4156 - val_acc: 0.8224
Epoch 40/100
588/588 [==============================] - 682s - loss: 0.4525 - acc: 0.8161 - val_loss: 0.4128 - val_acc: 0.8344
Epoch 41/100
588/588 [==============================] - 681s - loss: 0.4544 - acc: 0.8221 - val_loss: 0.4127 - val_acc: 0.8335
Epoch 42/100
588/588 [==============================] - 681s - loss: 0.4563 - acc: 0.8204 - val_loss: 0.4126 - val_acc: 0.8294
Epoch 43/100
588/588 [==============================] - 681s - loss: 0.4621 - acc: 0.8208 - val_loss: 0.3961 - val_acc: 0.8405
Epoch 44/100
588/588 [==============================] - 680s - loss: 0.4597 - acc: 0.8221 - val_loss: 0.3998 - val_acc: 0.8417
Epoch 45/100
588/588 [==============================] - 680s - loss: 0.4572 - acc: 0.8250 - val_loss: 0.4056 - val_acc: 0.8315
Epoch 46/100
588/588 [==============================] - 681s - loss: 0.4573 - acc: 0.8225 - val_loss: 0.3958 - val_acc: 0.8457
Epoch 47/100
588/588 [==============================] - 681s - loss: 0.4433 - acc: 0.8331 - val_loss: 0.4055 - val_acc: 0.8313
Epoch 48/100
588/588 [==============================] - 681s - loss: 0.4583 - acc: 0.8223 - val_loss: 0.4031 - val_acc: 0.8451
Epoch 49/100
588/588 [==============================] - 682s - loss: 0.4524 - acc: 0.8261 - val_loss: 0.4045 - val_acc: 0.8428
Epoch 50/100
588/588 [==============================] - 680s - loss: 0.4434 - acc: 0.8344 - val_loss: 0.3944 - val_acc: 0.8466
Epoch 51/100
588/588 [==============================] - 680s - loss: 0.4519 - acc: 0.8244 - val_loss: 0.3911 - val_acc: 0.8468
Epoch 52/100
588/588 [==============================] - 680s - loss: 0.4404 - acc: 0.8335 - val_loss: 0.3938 - val_acc: 0.8429
Epoch 53/100
588/588 [==============================] - 682s - loss: 0.4288 - acc: 0.8344 - val_loss: 0.3888 - val_acc: 0.8448
Epoch 54/100
588/588 [==============================] - 680s - loss: 0.4507 - acc: 0.8214 - val_loss: 0.3891 - val_acc: 0.8450
Epoch 55/100
588/588 [==============================] - 681s - loss: 0.4518 - acc: 0.8197 - val_loss: 0.3901 - val_acc: 0.8487
Epoch 56/100
588/588 [==============================] - 680s - loss: 0.4391 - acc: 0.8329 - val_loss: 0.3883 - val_acc: 0.8459
Epoch 57/100
588/588 [==============================] - 681s - loss: 0.4397 - acc: 0.8327 - val_loss: 0.3937 - val_acc: 0.8396
Epoch 58/100
588/588 [==============================] - 681s - loss: 0.4527 - acc: 0.8231 - val_loss: 0.3931 - val_acc: 0.8441
Epoch 59/100
588/588 [==============================] - 679s - loss: 0.4372 - acc: 0.8355 - val_loss: 0.3938 - val_acc: 0.8445
Epoch 60/100
588/588 [==============================] - 677s - loss: 0.4349 - acc: 0.8342 - val_loss: 0.3844 - val_acc: 0.8487
Epoch 61/100
588/588 [==============================] - 679s - loss: 0.4327 - acc: 0.8342 - val_loss: 0.3858 - val_acc: 0.8462
Epoch 62/100
588/588 [==============================] - 680s - loss: 0.4340 - acc: 0.8304 - val_loss: 0.3930 - val_acc: 0.8472
Epoch 63/100
588/588 [==============================] - 679s - loss: 0.4346 - acc: 0.8331 - val_loss: 0.3814 - val_acc: 0.8511
Epoch 64/100
588/588 [==============================] - 677s - loss: 0.4416 - acc: 0.8272 - val_loss: 0.3827 - val_acc: 0.8500
Epoch 65/100
588/588 [==============================] - 679s - loss: 0.4284 - acc: 0.8316 - val_loss: 0.3836 - val_acc: 0.8498
Epoch 66/100
588/588 [==============================] - 679s - loss: 0.4465 - acc: 0.8278 - val_loss: 0.3856 - val_acc: 0.8483
Epoch 67/100
588/588 [==============================] - 680s - loss: 0.4375 - acc: 0.8310 - val_loss: 0.3876 - val_acc: 0.8547
Epoch 68/100
588/588 [==============================] - 679s - loss: 0.4281 - acc: 0.8316 - val_loss: 0.3779 - val_acc: 0.8585
Epoch 69/100
588/588 [==============================] - 679s - loss: 0.4374 - acc: 0.8293 - val_loss: 0.3819 - val_acc: 0.8538
Epoch 70/100
588/588 [==============================] - 680s - loss: 0.4380 - acc: 0.8284 - val_loss: 0.3793 - val_acc: 0.8581
Epoch 71/100
588/588 [==============================] - 680s - loss: 0.4310 - acc: 0.8374 - val_loss: 0.3796 - val_acc: 0.8591
Epoch 72/100
588/588 [==============================] - 678s - loss: 0.4309 - acc: 0.8312 - val_loss: 0.3803 - val_acc: 0.8503
Epoch 73/100
588/588 [==============================] - 678s - loss: 0.4254 - acc: 0.8367 - val_loss: 0.3741 - val_acc: 0.8610
Epoch 74/100
588/588 [==============================] - 679s - loss: 0.4375 - acc: 0.8274 - val_loss: 0.3816 - val_acc: 0.8541
Epoch 75/100
588/588 [==============================] - 680s - loss: 0.4248 - acc: 0.8318 - val_loss: 0.3764 - val_acc: 0.8588
Epoch 76/100
588/588 [==============================] - 680s - loss: 0.4276 - acc: 0.8316 - val_loss: 0.3815 - val_acc: 0.8561
Epoch 77/100
588/588 [==============================] - 681s - loss: 0.4232 - acc: 0.8367 - val_loss: 0.3731 - val_acc: 0.8570
Epoch 78/100
588/588 [==============================] - 679s - loss: 0.4244 - acc: 0.8312 - val_loss: 0.3801 - val_acc: 0.8575
Epoch 79/100
588/588 [==============================] - 677s - loss: 0.4334 - acc: 0.8263 - val_loss: 0.3805 - val_acc: 0.8586
Epoch 80/100
588/588 [==============================] - 679s - loss: 0.4394 - acc: 0.8231 - val_loss: 0.3953 - val_acc: 0.8524
Epoch 81/100
588/588 [==============================] - 679s - loss: 0.4325 - acc: 0.8304 - val_loss: 0.3759 - val_acc: 0.8608
Epoch 82/100
588/588 [==============================] - 680s - loss: 0.4102 - acc: 0.8412 - val_loss: 0.3773 - val_acc: 0.8597
Epoch 83/100
588/588 [==============================] - 680s - loss: 0.4208 - acc: 0.8327 - val_loss: 0.3785 - val_acc: 0.8525
Epoch 84/100
588/588 [==============================] - 680s - loss: 0.4182 - acc: 0.8323 - val_loss: 0.3774 - val_acc: 0.8547
Epoch 85/100
588/588 [==============================] - 679s - loss: 0.4274 - acc: 0.8270 - val_loss: 0.3753 - val_acc: 0.8560

