(parkinson) xxx@xxx:~/xxx/dream_parkinsons/py/dream_parkinsons/appsâŸ« python train_dnn.py --dataset="/home/xxx/xxx/" --output_directory="/home/xxx/xxx/" --fraction_of_data_set=0.2 --num_units=32 --n_jobs=2 --num_epochs=100 --batch_size=8 --signal=return --dropout=0.6 --do_train
Using TensorFlow backend.
INFO: Args are: {'create_submission_file': False, 'do_evaluate': False, 'attention_dropout': 0.2, 'fraction_of_data_set': 0.2, 'n_jobs': 2, 'signal': 'return', 'do_train': True, 'dropout': 0.6, 'batch_size': 8, 'output_directory': '/home/xxx/xxx/', 'test_dataset': '/xxx/dream-test', 'per_walk_models': './models/outbound.h5,./models/rest.h5,./models/return.h5', 'num_units': 32, 'missing_dataset': '/xxx/xxx/dream-missing', 'num_epochs': 100, 'validation_set_fraction': 0.3, 'dataset': '/home/xxx/xxx/', 'seed': 909, 'load_existing': '', 'supplemental_dataset': '/xxx/dream-test'}
INFO: Built generators with 23521 training samples and 10080 validation samples. We are using 4704 / 23521 for training and 10080 / 10080 for validation.
INFO: Started training feature extraction.
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to
====================================================================================================
input_1 (InputLayer)             (None, None, 13)      0
____________________________________________________________________________________________________
masking_1 (Masking)              (None, None, 13)      0           input_1[0][0]
____________________________________________________________________________________________________
lambda_1 (Lambda)                (None, 1, None, 13)   0           masking_1[0][0]
____________________________________________________________________________________________________
conv2d_2 (Conv2D)                (None, 32, None, 13)  320         lambda_1[0][0]
____________________________________________________________________________________________________
activation_1 (Activation)        (None, 32, None, 13)  0           conv2d_2[0][0]
____________________________________________________________________________________________________
average_pooling2d_1 (AveragePool (None, 1, None, 13)   0           lambda_1[0][0]
____________________________________________________________________________________________________
conv2d_3 (Conv2D)                (None, 32, None, 13)  9248        activation_1[0][0]
____________________________________________________________________________________________________
conv2d_1 (Conv2D)                (None, 32, None, 13)  64          average_pooling2d_1[0][0]
____________________________________________________________________________________________________
average_pooling2d_2 (AveragePool (None, 32, None, 13)  0           conv2d_3[0][0]
____________________________________________________________________________________________________
add_1 (Add)                      (None, 32, None, 13)  0           conv2d_1[0][0]
                                                                   average_pooling2d_2[0][0]
____________________________________________________________________________________________________
batch_normalization_1 (BatchNorm (None, 32, None, 13)  128         add_1[0][0]
____________________________________________________________________________________________________
activation_2 (Activation)        (None, 32, None, 13)  0           batch_normalization_1[0][0]
____________________________________________________________________________________________________
conv2d_4 (Conv2D)                (None, 32, None, 13)  9248        activation_2[0][0]
____________________________________________________________________________________________________
batch_normalization_2 (BatchNorm (None, 32, None, 13)  128         conv2d_4[0][0]
____________________________________________________________________________________________________
activation_3 (Activation)        (None, 32, None, 13)  0           batch_normalization_2[0][0]
____________________________________________________________________________________________________
conv2d_6 (Conv2D)                (None, 32, None, 13)  1056        add_1[0][0]
____________________________________________________________________________________________________
conv2d_5 (Conv2D)                (None, 32, None, 13)  9248        activation_3[0][0]
____________________________________________________________________________________________________
average_pooling2d_4 (AveragePool (None, 32, None, 13)  0           conv2d_6[0][0]
____________________________________________________________________________________________________
average_pooling2d_3 (AveragePool (None, 32, None, 13)  0           conv2d_5[0][0]
____________________________________________________________________________________________________
add_2 (Add)                      (None, 32, None, 13)  0           average_pooling2d_4[0][0]
                                                                   average_pooling2d_3[0][0]
____________________________________________________________________________________________________
batch_normalization_3 (BatchNorm (None, 32, None, 13)  128         add_2[0][0]
____________________________________________________________________________________________________
activation_4 (Activation)        (None, 32, None, 13)  0           batch_normalization_3[0][0]
____________________________________________________________________________________________________
conv2d_7 (Conv2D)                (None, 32, None, 13)  9248        activation_4[0][0]
____________________________________________________________________________________________________
batch_normalization_4 (BatchNorm (None, 32, None, 13)  128         conv2d_7[0][0]
____________________________________________________________________________________________________
activation_5 (Activation)        (None, 32, None, 13)  0           batch_normalization_4[0][0]
____________________________________________________________________________________________________
conv2d_9 (Conv2D)                (None, 32, None, 13)  1056        add_2[0][0]
____________________________________________________________________________________________________
conv2d_8 (Conv2D)                (None, 32, None, 13)  9248        activation_5[0][0]
____________________________________________________________________________________________________
average_pooling2d_6 (AveragePool (None, 32, None, 13)  0           conv2d_9[0][0]
____________________________________________________________________________________________________
average_pooling2d_5 (AveragePool (None, 32, None, 13)  0           conv2d_8[0][0]
____________________________________________________________________________________________________
add_3 (Add)                      (None, 32, None, 13)  0           average_pooling2d_6[0][0]
                                                                   average_pooling2d_5[0][0]
____________________________________________________________________________________________________
batch_normalization_5 (BatchNorm (None, 32, None, 13)  128         add_3[0][0]
____________________________________________________________________________________________________
activation_6 (Activation)        (None, 32, None, 13)  0           batch_normalization_5[0][0]
____________________________________________________________________________________________________
conv2d_10 (Conv2D)               (None, 32, None, 13)  9248        activation_6[0][0]
____________________________________________________________________________________________________
batch_normalization_6 (BatchNorm (None, 32, None, 13)  128         conv2d_10[0][0]
____________________________________________________________________________________________________
activation_7 (Activation)        (None, 32, None, 13)  0           batch_normalization_6[0][0]
____________________________________________________________________________________________________
conv2d_12 (Conv2D)               (None, 32, None, 13)  1056        add_3[0][0]
____________________________________________________________________________________________________
conv2d_11 (Conv2D)               (None, 32, None, 13)  9248        activation_7[0][0]
____________________________________________________________________________________________________
average_pooling2d_8 (AveragePool (None, 32, None, 13)  0           conv2d_12[0][0]
____________________________________________________________________________________________________
average_pooling2d_7 (AveragePool (None, 32, None, 13)  0           conv2d_11[0][0]
____________________________________________________________________________________________________
add_4 (Add)                      (None, 32, None, 13)  0           average_pooling2d_8[0][0]
                                                                   average_pooling2d_7[0][0]
____________________________________________________________________________________________________
batch_normalization_7 (BatchNorm (None, 32, None, 13)  128         add_4[0][0]
____________________________________________________________________________________________________
activation_8 (Activation)        (None, 32, None, 13)  0           batch_normalization_7[0][0]
____________________________________________________________________________________________________
conv2d_13 (Conv2D)               (None, 32, None, 13)  9248        activation_8[0][0]
____________________________________________________________________________________________________
batch_normalization_8 (BatchNorm (None, 32, None, 13)  128         conv2d_13[0][0]
____________________________________________________________________________________________________
activation_9 (Activation)        (None, 32, None, 13)  0           batch_normalization_8[0][0]
____________________________________________________________________________________________________
conv2d_15 (Conv2D)               (None, 32, None, 13)  1056        add_4[0][0]
____________________________________________________________________________________________________
conv2d_14 (Conv2D)               (None, 32, None, 13)  9248        activation_9[0][0]
____________________________________________________________________________________________________
average_pooling2d_10 (AveragePoo (None, 32, None, 13)  0           conv2d_15[0][0]
____________________________________________________________________________________________________
average_pooling2d_9 (AveragePool (None, 32, None, 13)  0           conv2d_14[0][0]
____________________________________________________________________________________________________
add_5 (Add)                      (None, 32, None, 13)  0           average_pooling2d_10[0][0]
                                                                   average_pooling2d_9[0][0]
____________________________________________________________________________________________________
batch_normalization_9 (BatchNorm (None, 32, None, 13)  128         add_5[0][0]
____________________________________________________________________________________________________
activation_10 (Activation)       (None, 32, None, 13)  0           batch_normalization_9[0][0]
____________________________________________________________________________________________________
conv2d_16 (Conv2D)               (None, 32, None, 13)  9248        activation_10[0][0]
____________________________________________________________________________________________________
batch_normalization_10 (BatchNor (None, 32, None, 13)  128         conv2d_16[0][0]
____________________________________________________________________________________________________
activation_11 (Activation)       (None, 32, None, 13)  0           batch_normalization_10[0][0]
____________________________________________________________________________________________________
conv2d_18 (Conv2D)               (None, 32, None, 13)  1056        add_5[0][0]
____________________________________________________________________________________________________
conv2d_17 (Conv2D)               (None, 32, None, 13)  9248        activation_11[0][0]
____________________________________________________________________________________________________
average_pooling2d_12 (AveragePoo (None, 32, None, 13)  0           conv2d_18[0][0]
____________________________________________________________________________________________________
average_pooling2d_11 (AveragePoo (None, 32, None, 13)  0           conv2d_17[0][0]
____________________________________________________________________________________________________
add_6 (Add)                      (None, 32, None, 13)  0           average_pooling2d_12[0][0]
                                                                   average_pooling2d_11[0][0]
____________________________________________________________________________________________________
batch_normalization_11 (BatchNor (None, 32, None, 13)  128         add_6[0][0]
____________________________________________________________________________________________________
activation_12 (Activation)       (None, 32, None, 13)  0           batch_normalization_11[0][0]
____________________________________________________________________________________________________
conv2d_19 (Conv2D)               (None, 1, None, 13)   289         activation_12[0][0]
____________________________________________________________________________________________________
lambda_2 (Lambda)                (None, None, 13)      0           conv2d_19[0][0]
____________________________________________________________________________________________________
bidirectional_1 (Bidirectional)  (None, None, 64)      11776       lambda_2[0][0]
____________________________________________________________________________________________________
soft_attention_1 (SoftAttention) (None, 64)            4224        bidirectional_1[0][0]
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 32)            2080        soft_attention_1[0][0]
____________________________________________________________________________________________________
batch_normalization_12 (BatchNor (None, 32)            128         dense_1[0][0]
____________________________________________________________________________________________________
activation_13 (Activation)       (None, 32)            0           batch_normalization_12[0][0]
____________________________________________________________________________________________________
dropout_1 (Dropout)              (None, 32)            0           activation_13[0][0]
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 32)            1056        dropout_1[0][0]
____________________________________________________________________________________________________
batch_normalization_13 (BatchNor (None, 32)            128         dense_2[0][0]
____________________________________________________________________________________________________
activation_14 (Activation)       (None, 32)            0           batch_normalization_13[0][0]
____________________________________________________________________________________________________
input_2 (InputLayer)             (None, 1)             0
____________________________________________________________________________________________________
input_3 (InputLayer)             (None, 1)             0
____________________________________________________________________________________________________
dropout_2 (Dropout)              (None, 32)            0           activation_14[0][0]
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 34)            0           input_2[0][0]
                                                                   input_3[0][0]
                                                                   dropout_2[0][0]
____________________________________________________________________________________________________
discriminator_output (Dense)     (None, 1)             35          concatenate_1[0][0]
====================================================================================================
Total params: 128,516
Trainable params: 127,684
Non-trainable params: 832
____________________________________________________________________________________________________
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to
====================================================================================================
input_1 (InputLayer)             (None, None, 13)      0
____________________________________________________________________________________________________
masking_1 (Masking)              (None, None, 13)      0           input_1[0][0]
____________________________________________________________________________________________________
lambda_1 (Lambda)                (None, 1, None, 13)   0           masking_1[0][0]
____________________________________________________________________________________________________
conv2d_2 (Conv2D)                (None, 32, None, 13)  320         lambda_1[0][0]
____________________________________________________________________________________________________
activation_1 (Activation)        (None, 32, None, 13)  0           conv2d_2[0][0]
____________________________________________________________________________________________________
average_pooling2d_1 (AveragePool (None, 1, None, 13)   0           lambda_1[0][0]
____________________________________________________________________________________________________
conv2d_3 (Conv2D)                (None, 32, None, 13)  9248        activation_1[0][0]
____________________________________________________________________________________________________
conv2d_1 (Conv2D)                (None, 32, None, 13)  64          average_pooling2d_1[0][0]
____________________________________________________________________________________________________
average_pooling2d_2 (AveragePool (None, 32, None, 13)  0           conv2d_3[0][0]
____________________________________________________________________________________________________
add_1 (Add)                      (None, 32, None, 13)  0           conv2d_1[0][0]
                                                                   average_pooling2d_2[0][0]
____________________________________________________________________________________________________
batch_normalization_1 (BatchNorm (None, 32, None, 13)  128         add_1[0][0]
____________________________________________________________________________________________________
activation_2 (Activation)        (None, 32, None, 13)  0           batch_normalization_1[0][0]
____________________________________________________________________________________________________
conv2d_4 (Conv2D)                (None, 32, None, 13)  9248        activation_2[0][0]
____________________________________________________________________________________________________
batch_normalization_2 (BatchNorm (None, 32, None, 13)  128         conv2d_4[0][0]
____________________________________________________________________________________________________
activation_3 (Activation)        (None, 32, None, 13)  0           batch_normalization_2[0][0]
____________________________________________________________________________________________________
conv2d_6 (Conv2D)                (None, 32, None, 13)  1056        add_1[0][0]
____________________________________________________________________________________________________
conv2d_5 (Conv2D)                (None, 32, None, 13)  9248        activation_3[0][0]
____________________________________________________________________________________________________
average_pooling2d_4 (AveragePool (None, 32, None, 13)  0           conv2d_6[0][0]
____________________________________________________________________________________________________
average_pooling2d_3 (AveragePool (None, 32, None, 13)  0           conv2d_5[0][0]
____________________________________________________________________________________________________
add_2 (Add)                      (None, 32, None, 13)  0           average_pooling2d_4[0][0]
                                                                   average_pooling2d_3[0][0]
____________________________________________________________________________________________________
batch_normalization_3 (BatchNorm (None, 32, None, 13)  128         add_2[0][0]
____________________________________________________________________________________________________
activation_4 (Activation)        (None, 32, None, 13)  0           batch_normalization_3[0][0]
____________________________________________________________________________________________________
conv2d_7 (Conv2D)                (None, 32, None, 13)  9248        activation_4[0][0]
____________________________________________________________________________________________________
batch_normalization_4 (BatchNorm (None, 32, None, 13)  128         conv2d_7[0][0]
____________________________________________________________________________________________________
activation_5 (Activation)        (None, 32, None, 13)  0           batch_normalization_4[0][0]
____________________________________________________________________________________________________
conv2d_9 (Conv2D)                (None, 32, None, 13)  1056        add_2[0][0]
____________________________________________________________________________________________________
conv2d_8 (Conv2D)                (None, 32, None, 13)  9248        activation_5[0][0]
____________________________________________________________________________________________________
average_pooling2d_6 (AveragePool (None, 32, None, 13)  0           conv2d_9[0][0]
____________________________________________________________________________________________________
average_pooling2d_5 (AveragePool (None, 32, None, 13)  0           conv2d_8[0][0]
____________________________________________________________________________________________________
add_3 (Add)                      (None, 32, None, 13)  0           average_pooling2d_6[0][0]
                                                                   average_pooling2d_5[0][0]
____________________________________________________________________________________________________
batch_normalization_5 (BatchNorm (None, 32, None, 13)  128         add_3[0][0]
____________________________________________________________________________________________________
activation_6 (Activation)        (None, 32, None, 13)  0           batch_normalization_5[0][0]
____________________________________________________________________________________________________
conv2d_10 (Conv2D)               (None, 32, None, 13)  9248        activation_6[0][0]
____________________________________________________________________________________________________
batch_normalization_6 (BatchNorm (None, 32, None, 13)  128         conv2d_10[0][0]
____________________________________________________________________________________________________
activation_7 (Activation)        (None, 32, None, 13)  0           batch_normalization_6[0][0]
____________________________________________________________________________________________________
conv2d_12 (Conv2D)               (None, 32, None, 13)  1056        add_3[0][0]
____________________________________________________________________________________________________
conv2d_11 (Conv2D)               (None, 32, None, 13)  9248        activation_7[0][0]
____________________________________________________________________________________________________
average_pooling2d_8 (AveragePool (None, 32, None, 13)  0           conv2d_12[0][0]
____________________________________________________________________________________________________
average_pooling2d_7 (AveragePool (None, 32, None, 13)  0           conv2d_11[0][0]
____________________________________________________________________________________________________
add_4 (Add)                      (None, 32, None, 13)  0           average_pooling2d_8[0][0]
                                                                   average_pooling2d_7[0][0]
____________________________________________________________________________________________________
batch_normalization_7 (BatchNorm (None, 32, None, 13)  128         add_4[0][0]
____________________________________________________________________________________________________
activation_8 (Activation)        (None, 32, None, 13)  0           batch_normalization_7[0][0]
____________________________________________________________________________________________________
conv2d_13 (Conv2D)               (None, 32, None, 13)  9248        activation_8[0][0]
____________________________________________________________________________________________________
batch_normalization_8 (BatchNorm (None, 32, None, 13)  128         conv2d_13[0][0]
____________________________________________________________________________________________________
activation_9 (Activation)        (None, 32, None, 13)  0           batch_normalization_8[0][0]
____________________________________________________________________________________________________
conv2d_15 (Conv2D)               (None, 32, None, 13)  1056        add_4[0][0]
____________________________________________________________________________________________________
conv2d_14 (Conv2D)               (None, 32, None, 13)  9248        activation_9[0][0]
____________________________________________________________________________________________________
average_pooling2d_10 (AveragePoo (None, 32, None, 13)  0           conv2d_15[0][0]
____________________________________________________________________________________________________
average_pooling2d_9 (AveragePool (None, 32, None, 13)  0           conv2d_14[0][0]
____________________________________________________________________________________________________
add_5 (Add)                      (None, 32, None, 13)  0           average_pooling2d_10[0][0]
                                                                   average_pooling2d_9[0][0]
____________________________________________________________________________________________________
batch_normalization_9 (BatchNorm (None, 32, None, 13)  128         add_5[0][0]
____________________________________________________________________________________________________
activation_10 (Activation)       (None, 32, None, 13)  0           batch_normalization_9[0][0]
____________________________________________________________________________________________________
conv2d_16 (Conv2D)               (None, 32, None, 13)  9248        activation_10[0][0]
____________________________________________________________________________________________________
batch_normalization_10 (BatchNor (None, 32, None, 13)  128         conv2d_16[0][0]
____________________________________________________________________________________________________
activation_11 (Activation)       (None, 32, None, 13)  0           batch_normalization_10[0][0]
____________________________________________________________________________________________________
conv2d_18 (Conv2D)               (None, 32, None, 13)  1056        add_5[0][0]
____________________________________________________________________________________________________
conv2d_17 (Conv2D)               (None, 32, None, 13)  9248        activation_11[0][0]
____________________________________________________________________________________________________
average_pooling2d_12 (AveragePoo (None, 32, None, 13)  0           conv2d_18[0][0]
____________________________________________________________________________________________________
average_pooling2d_11 (AveragePoo (None, 32, None, 13)  0           conv2d_17[0][0]
____________________________________________________________________________________________________
add_6 (Add)                      (None, 32, None, 13)  0           average_pooling2d_12[0][0]
                                                                   average_pooling2d_11[0][0]
____________________________________________________________________________________________________
batch_normalization_11 (BatchNor (None, 32, None, 13)  128         add_6[0][0]
____________________________________________________________________________________________________
activation_12 (Activation)       (None, 32, None, 13)  0           batch_normalization_11[0][0]
____________________________________________________________________________________________________
conv2d_19 (Conv2D)               (None, 1, None, 13)   289         activation_12[0][0]
____________________________________________________________________________________________________
lambda_2 (Lambda)                (None, None, 13)      0           conv2d_19[0][0]
____________________________________________________________________________________________________
bidirectional_1 (Bidirectional)  (None, None, 64)      11776       lambda_2[0][0]
____________________________________________________________________________________________________
soft_attention_1 (SoftAttention) (None, 64)            4224        bidirectional_1[0][0]
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 32)            2080        soft_attention_1[0][0]
____________________________________________________________________________________________________
batch_normalization_12 (BatchNor (None, 32)            128         dense_1[0][0]
____________________________________________________________________________________________________
activation_13 (Activation)       (None, 32)            0           batch_normalization_12[0][0]
____________________________________________________________________________________________________
dropout_1 (Dropout)              (None, 32)            0           activation_13[0][0]
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 32)            1056        dropout_1[0][0]
____________________________________________________________________________________________________
batch_normalization_13 (BatchNor (None, 32)            128         dense_2[0][0]
____________________________________________________________________________________________________
activation_14 (Activation)       (None, 32)            0           batch_normalization_13[0][0]
____________________________________________________________________________________________________
input_2 (InputLayer)             (None, 1)             0
____________________________________________________________________________________________________
input_3 (InputLayer)             (None, 1)             0
____________________________________________________________________________________________________
dropout_2 (Dropout)              (None, 32)            0           activation_14[0][0]
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 34)            0           input_2[0][0]
                                                                   input_3[0][0]
                                                                   dropout_2[0][0]
____________________________________________________________________________________________________
discriminator_output (Dense)     (None, 1)             35          concatenate_1[0][0]
====================================================================================================
Total params: 128,516
Trainable params: 127,684
Non-trainable params: 832
____________________________________________________________________________________________________
Epoch 1/100
2017-10-02 16:11:22.509883: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-10-02 16:11:22.509916: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-10-02 16:11:22.509926: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-10-02 16:11:22.509933: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed upCPU computations.
2017-10-02 16:11:22.509941: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-10-02 16:11:23.271727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:
name: GeForce GTX 970
major: 5 minor: 2 memoryClockRate (GHz) 1.253
pciBusID 0000:01:00.0
Total memory: 3.94GiB
Free memory: 3.88GiB
2017-10-02 16:11:23.271754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0
2017-10-02 16:11:23.271758: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y
2017-10-02 16:11:23.271764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 970, pci bus id: 0000:01:00.0)
588/588 [==============================] - 733s - loss: 0.8418 - acc: 0.5285 - val_loss: 0.6890 - val_acc: 0.5315
Epoch 2/100
588/588 [==============================] - 637s - loss: 0.6855 - acc: 0.6165 - val_loss: 0.6815 - val_acc: 0.5330
Epoch 3/100
588/588 [==============================] - 625s - loss: 0.6309 - acc: 0.6471 - val_loss: 0.6703 - val_acc: 0.5316
Epoch 4/100
588/588 [==============================] - 623s - loss: 0.6117 - acc: 0.6522 - val_loss: 0.6574 - val_acc: 0.5278
Epoch 5/100
588/588 [==============================] - 623s - loss: 0.5948 - acc: 0.6597 - val_loss: 0.6481 - val_acc: 0.5271
Epoch 6/100
588/588 [==============================] - 618s - loss: 0.5836 - acc: 0.6726 - val_loss: 0.6415 - val_acc: 0.5201
Epoch 7/100
588/588 [==============================] - 621s - loss: 0.5741 - acc: 0.6730 - val_loss: 0.6217 - val_acc: 0.5694
Epoch 8/100
588/588 [==============================] - 619s - loss: 0.5644 - acc: 0.6901 - val_loss: 0.6301 - val_acc: 0.5538
Epoch 9/100
588/588 [==============================] - 611s - loss: 0.5632 - acc: 0.7092 - val_loss: 0.6185 - val_acc: 0.5682
Epoch 10/100
588/588 [==============================] - 618s - loss: 0.5440 - acc: 0.7251 - val_loss: 0.5926 - val_acc: 0.6263
Epoch 11/100
588/588 [==============================] - 619s - loss: 0.5468 - acc: 0.7360 - val_loss: 0.5897 - val_acc: 0.6430
Epoch 12/100
588/588 [==============================] - 616s - loss: 0.5329 - acc: 0.7430 - val_loss: 0.5748 - val_acc: 0.6833
Epoch 13/100
588/588 [==============================] - 616s - loss: 0.5250 - acc: 0.7566 - val_loss: 0.5729 - val_acc: 0.6880
Epoch 14/100
588/588 [==============================] - 613s - loss: 0.5139 - acc: 0.7687 - val_loss: 0.5753 - val_acc: 0.6784
Epoch 15/100
588/588 [==============================] - 613s - loss: 0.5219 - acc: 0.7636 - val_loss: 0.5781 - val_acc: 0.6907
Epoch 16/100
588/588 [==============================] - 616s - loss: 0.5151 - acc: 0.7836 - val_loss: 0.5670 - val_acc: 0.7049
Epoch 17/100
588/588 [==============================] - 619s - loss: 0.5097 - acc: 0.7823 - val_loss: 0.5701 - val_acc: 0.7067
Epoch 18/100
588/588 [==============================] - 618s - loss: 0.5038 - acc: 0.7834 - val_loss: 0.5763 - val_acc: 0.6881
Epoch 19/100
588/588 [==============================] - 619s - loss: 0.4965 - acc: 0.7821 - val_loss: 0.5629 - val_acc: 0.7019
Epoch 20/100
588/588 [==============================] - 610s - loss: 0.4938 - acc: 0.7955 - val_loss: 0.5626 - val_acc: 0.7215
Epoch 21/100
588/588 [==============================] - 619s - loss: 0.4735 - acc: 0.8131 - val_loss: 0.5694 - val_acc: 0.7031
Epoch 22/100
588/588 [==============================] - 615s - loss: 0.4710 - acc: 0.8031 - val_loss: 0.5576 - val_acc: 0.7174
Epoch 23/100
588/588 [==============================] - 618s - loss: 0.4847 - acc: 0.8038 - val_loss: 0.5412 - val_acc: 0.7472
Epoch 24/100
588/588 [==============================] - 617s - loss: 0.4749 - acc: 0.8114 - val_loss: 0.5567 - val_acc: 0.7183
Epoch 25/100
588/588 [==============================] - 612s - loss: 0.4718 - acc: 0.8087 - val_loss: 0.5247 - val_acc: 0.7580
Epoch 26/100
588/588 [==============================] - 612s - loss: 0.4590 - acc: 0.8163 - val_loss: 0.5248 - val_acc: 0.7605
Epoch 27/100
588/588 [==============================] - 614s - loss: 0.4565 - acc: 0.8174 - val_loss: 0.5307 - val_acc: 0.7474
Epoch 28/100
588/588 [==============================] - 610s - loss: 0.4511 - acc: 0.8204 - val_loss: 0.5131 - val_acc: 0.7664
Epoch 29/100
588/588 [==============================] - 616s - loss: 0.4521 - acc: 0.8142 - val_loss: 0.5046 - val_acc: 0.7762
Epoch 30/100
588/588 [==============================] - 614s - loss: 0.4501 - acc: 0.8180 - val_loss: 0.5210 - val_acc: 0.7603
Epoch 31/100
588/588 [==============================] - 618s - loss: 0.4545 - acc: 0.8131 - val_loss: 0.5171 - val_acc: 0.7626
Epoch 32/100
588/588 [==============================] - 611s - loss: 0.4397 - acc: 0.8236 - val_loss: 0.5000 - val_acc: 0.7805
Epoch 33/100
588/588 [==============================] - 610s - loss: 0.4192 - acc: 0.8408 - val_loss: 0.5063 - val_acc: 0.7718
Epoch 34/100
588/588 [==============================] - 615s - loss: 0.4346 - acc: 0.8225 - val_loss: 0.4952 - val_acc: 0.7791
Epoch 35/100
588/588 [==============================] - 610s - loss: 0.4164 - acc: 0.8372 - val_loss: 0.5153 - val_acc: 0.7661
Epoch 36/100
588/588 [==============================] - 617s - loss: 0.4342 - acc: 0.8206 - val_loss: 0.4828 - val_acc: 0.7865
Epoch 37/100
588/588 [==============================] - 611s - loss: 0.4184 - acc: 0.8293 - val_loss: 0.5074 - val_acc: 0.7739
Epoch 38/100
588/588 [==============================] - 614s - loss: 0.4124 - acc: 0.8325 - val_loss: 0.4870 - val_acc: 0.7844
Epoch 39/100
 67/588 [==>...........................] - ETA: 342s - loss: 0.4229 - acc: 0.8451
KeyboardInterrupt
