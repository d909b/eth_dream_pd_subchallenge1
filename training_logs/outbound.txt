(parkinson) xxx@xxx:~/xxx/dream_parkinsons/py/dream_parkinsons/appsâŸ« python train_dnn.py --dataset="/home/xxx/xxx/" --output_directory="/home/xxx/xxx/" --fraction_of_data_set=0.2 --num_units=32 --n_jobs=2 --num_epochs=100 --batch_size=8 --signal=outbound
Using TensorFlow backend.
INFO: Built generators with 23521 training samples and 10080 validation samples. We are using 4704 / 23521 for training and 10080 / 10080 for validation.
INFO: Started training feature extraction.
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to
====================================================================================================
input_1 (InputLayer)             (None, None, 13)      0
____________________________________________________________________________________________________
masking_1 (Masking)              (None, None, 13)      0           input_1[0][0]
____________________________________________________________________________________________________
lambda_1 (Lambda)                (None, 1, None, 13)   0           masking_1[0][0]
____________________________________________________________________________________________________
conv2d_2 (Conv2D)                (None, 32, None, 13)  320         lambda_1[0][0]
____________________________________________________________________________________________________
activation_1 (Activation)        (None, 32, None, 13)  0           conv2d_2[0][0]
____________________________________________________________________________________________________
average_pooling2d_1 (AveragePool (None, 1, None, 13)   0           lambda_1[0][0]
____________________________________________________________________________________________________
conv2d_3 (Conv2D)                (None, 32, None, 13)  9248        activation_1[0][0]
____________________________________________________________________________________________________
conv2d_1 (Conv2D)                (None, 32, None, 13)  64          average_pooling2d_1[0][0]
____________________________________________________________________________________________________
average_pooling2d_2 (AveragePool (None, 32, None, 13)  0           conv2d_3[0][0]
____________________________________________________________________________________________________
add_1 (Add)                      (None, 32, None, 13)  0           conv2d_1[0][0]
                                                                   average_pooling2d_2[0][0]
____________________________________________________________________________________________________
batch_normalization_1 (BatchNorm (None, 32, None, 13)  128         add_1[0][0]
____________________________________________________________________________________________________
activation_2 (Activation)        (None, 32, None, 13)  0           batch_normalization_1[0][0]
____________________________________________________________________________________________________
conv2d_4 (Conv2D)                (None, 32, None, 13)  9248        activation_2[0][0]
____________________________________________________________________________________________________
batch_normalization_2 (BatchNorm (None, 32, None, 13)  128         conv2d_4[0][0]
____________________________________________________________________________________________________
activation_3 (Activation)        (None, 32, None, 13)  0           batch_normalization_2[0][0]
____________________________________________________________________________________________________
conv2d_6 (Conv2D)                (None, 32, None, 13)  1056        add_1[0][0]
____________________________________________________________________________________________________
conv2d_5 (Conv2D)                (None, 32, None, 13)  9248        activation_3[0][0]
____________________________________________________________________________________________________
average_pooling2d_4 (AveragePool (None, 32, None, 13)  0           conv2d_6[0][0]
____________________________________________________________________________________________________
average_pooling2d_3 (AveragePool (None, 32, None, 13)  0           conv2d_5[0][0]
____________________________________________________________________________________________________
add_2 (Add)                      (None, 32, None, 13)  0           average_pooling2d_4[0][0]
                                                                   average_pooling2d_3[0][0]
____________________________________________________________________________________________________
batch_normalization_3 (BatchNorm (None, 32, None, 13)  128         add_2[0][0]
____________________________________________________________________________________________________
activation_4 (Activation)        (None, 32, None, 13)  0           batch_normalization_3[0][0]
____________________________________________________________________________________________________
conv2d_7 (Conv2D)                (None, 32, None, 13)  9248        activation_4[0][0]
____________________________________________________________________________________________________
batch_normalization_4 (BatchNorm (None, 32, None, 13)  128         conv2d_7[0][0]
____________________________________________________________________________________________________
activation_5 (Activation)        (None, 32, None, 13)  0           batch_normalization_4[0][0]
____________________________________________________________________________________________________
conv2d_9 (Conv2D)                (None, 32, None, 13)  1056        add_2[0][0]
____________________________________________________________________________________________________
conv2d_8 (Conv2D)                (None, 32, None, 13)  9248        activation_5[0][0]
____________________________________________________________________________________________________
average_pooling2d_6 (AveragePool (None, 32, None, 13)  0           conv2d_9[0][0]
____________________________________________________________________________________________________
average_pooling2d_5 (AveragePool (None, 32, None, 13)  0           conv2d_8[0][0]
____________________________________________________________________________________________________
add_3 (Add)                      (None, 32, None, 13)  0           average_pooling2d_6[0][0]
                                                                   average_pooling2d_5[0][0]
____________________________________________________________________________________________________
batch_normalization_5 (BatchNorm (None, 32, None, 13)  128         add_3[0][0]
____________________________________________________________________________________________________
activation_6 (Activation)        (None, 32, None, 13)  0           batch_normalization_5[0][0]
____________________________________________________________________________________________________
conv2d_10 (Conv2D)               (None, 32, None, 13)  9248        activation_6[0][0]
____________________________________________________________________________________________________
batch_normalization_6 (BatchNorm (None, 32, None, 13)  128         conv2d_10[0][0]
____________________________________________________________________________________________________
activation_7 (Activation)        (None, 32, None, 13)  0           batch_normalization_6[0][0]
____________________________________________________________________________________________________
conv2d_12 (Conv2D)               (None, 32, None, 13)  1056        add_3[0][0]
____________________________________________________________________________________________________
conv2d_11 (Conv2D)               (None, 32, None, 13)  9248        activation_7[0][0]
____________________________________________________________________________________________________
average_pooling2d_8 (AveragePool (None, 32, None, 13)  0           conv2d_12[0][0]
____________________________________________________________________________________________________
average_pooling2d_7 (AveragePool (None, 32, None, 13)  0           conv2d_11[0][0]
____________________________________________________________________________________________________
add_4 (Add)                      (None, 32, None, 13)  0           average_pooling2d_8[0][0]
                                                                   average_pooling2d_7[0][0]
____________________________________________________________________________________________________
batch_normalization_7 (BatchNorm (None, 32, None, 13)  128         add_4[0][0]
____________________________________________________________________________________________________
activation_8 (Activation)        (None, 32, None, 13)  0           batch_normalization_7[0][0]
____________________________________________________________________________________________________
conv2d_13 (Conv2D)               (None, 32, None, 13)  9248        activation_8[0][0]
____________________________________________________________________________________________________
batch_normalization_8 (BatchNorm (None, 32, None, 13)  128         conv2d_13[0][0]
____________________________________________________________________________________________________
activation_9 (Activation)        (None, 32, None, 13)  0           batch_normalization_8[0][0]
____________________________________________________________________________________________________
conv2d_15 (Conv2D)               (None, 32, None, 13)  1056        add_4[0][0]
____________________________________________________________________________________________________
conv2d_14 (Conv2D)               (None, 32, None, 13)  9248        activation_9[0][0]
____________________________________________________________________________________________________
average_pooling2d_10 (AveragePoo (None, 32, None, 13)  0           conv2d_15[0][0]
____________________________________________________________________________________________________
average_pooling2d_9 (AveragePool (None, 32, None, 13)  0           conv2d_14[0][0]
____________________________________________________________________________________________________
add_5 (Add)                      (None, 32, None, 13)  0           average_pooling2d_10[0][0]
                                                                   average_pooling2d_9[0][0]
____________________________________________________________________________________________________
batch_normalization_9 (BatchNorm (None, 32, None, 13)  128         add_5[0][0]
____________________________________________________________________________________________________
activation_10 (Activation)       (None, 32, None, 13)  0           batch_normalization_9[0][0]
____________________________________________________________________________________________________
conv2d_16 (Conv2D)               (None, 32, None, 13)  9248        activation_10[0][0]
____________________________________________________________________________________________________
batch_normalization_10 (BatchNor (None, 32, None, 13)  128         conv2d_16[0][0]
____________________________________________________________________________________________________
activation_11 (Activation)       (None, 32, None, 13)  0           batch_normalization_10[0][0]
____________________________________________________________________________________________________
conv2d_18 (Conv2D)               (None, 32, None, 13)  1056        add_5[0][0]
____________________________________________________________________________________________________
conv2d_17 (Conv2D)               (None, 32, None, 13)  9248        activation_11[0][0]
____________________________________________________________________________________________________
average_pooling2d_12 (AveragePoo (None, 32, None, 13)  0           conv2d_18[0][0]
____________________________________________________________________________________________________
average_pooling2d_11 (AveragePoo (None, 32, None, 13)  0           conv2d_17[0][0]
____________________________________________________________________________________________________
add_6 (Add)                      (None, 32, None, 13)  0           average_pooling2d_12[0][0]
                                                                   average_pooling2d_11[0][0]
____________________________________________________________________________________________________
batch_normalization_11 (BatchNor (None, 32, None, 13)  128         add_6[0][0]
____________________________________________________________________________________________________
activation_12 (Activation)       (None, 32, None, 13)  0           batch_normalization_11[0][0]
____________________________________________________________________________________________________
conv2d_19 (Conv2D)               (None, 1, None, 13)   289         activation_12[0][0]
____________________________________________________________________________________________________
lambda_2 (Lambda)                (None, None, 13)      0           conv2d_19[0][0]
____________________________________________________________________________________________________
bidirectional_1 (Bidirectional)  (None, None, 64)      11776       lambda_2[0][0]
____________________________________________________________________________________________________
soft_attention_1 (SoftAttention) (None, 64)            4224        bidirectional_1[0][0]
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 32)            2080        soft_attention_1[0][0]
____________________________________________________________________________________________________
batch_normalization_12 (BatchNor (None, 32)            128         dense_1[0][0]
____________________________________________________________________________________________________
activation_13 (Activation)       (None, 32)            0           batch_normalization_12[0][0]
____________________________________________________________________________________________________
dropout_1 (Dropout)              (None, 32)            0           activation_13[0][0]
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 32)            1056        dropout_1[0][0]
____________________________________________________________________________________________________
batch_normalization_13 (BatchNor (None, 32)            128         dense_2[0][0]
____________________________________________________________________________________________________
activation_14 (Activation)       (None, 32)            0           batch_normalization_13[0][0]
____________________________________________________________________________________________________
input_2 (InputLayer)             (None, 1)             0
____________________________________________________________________________________________________
input_3 (InputLayer)             (None, 1)             0
____________________________________________________________________________________________________
dropout_2 (Dropout)              (None, 32)            0           activation_14[0][0]
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 34)            0           input_2[0][0]
                                                                   input_3[0][0]
                                                                   dropout_2[0][0]
____________________________________________________________________________________________________
discriminator_output (Dense)     (None, 1)             35          concatenate_1[0][0]
====================================================================================================
Total params: 128,516
Trainable params: 127,684
Non-trainable params: 832
____________________________________________________________________________________________________
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to
====================================================================================================
input_1 (InputLayer)             (None, None, 13)      0
____________________________________________________________________________________________________
masking_1 (Masking)              (None, None, 13)      0           input_1[0][0]
____________________________________________________________________________________________________
lambda_1 (Lambda)                (None, 1, None, 13)   0           masking_1[0][0]
____________________________________________________________________________________________________
conv2d_2 (Conv2D)                (None, 32, None, 13)  320         lambda_1[0][0]
____________________________________________________________________________________________________
activation_1 (Activation)        (None, 32, None, 13)  0           conv2d_2[0][0]
____________________________________________________________________________________________________
average_pooling2d_1 (AveragePool (None, 1, None, 13)   0           lambda_1[0][0]
____________________________________________________________________________________________________
conv2d_3 (Conv2D)                (None, 32, None, 13)  9248        activation_1[0][0]
____________________________________________________________________________________________________
conv2d_1 (Conv2D)                (None, 32, None, 13)  64          average_pooling2d_1[0][0]
____________________________________________________________________________________________________
average_pooling2d_2 (AveragePool (None, 32, None, 13)  0           conv2d_3[0][0]
____________________________________________________________________________________________________
add_1 (Add)                      (None, 32, None, 13)  0           conv2d_1[0][0]
                                                                   average_pooling2d_2[0][0]
____________________________________________________________________________________________________
batch_normalization_1 (BatchNorm (None, 32, None, 13)  128         add_1[0][0]
____________________________________________________________________________________________________
activation_2 (Activation)        (None, 32, None, 13)  0           batch_normalization_1[0][0]
____________________________________________________________________________________________________
conv2d_4 (Conv2D)                (None, 32, None, 13)  9248        activation_2[0][0]
____________________________________________________________________________________________________
batch_normalization_2 (BatchNorm (None, 32, None, 13)  128         conv2d_4[0][0]
____________________________________________________________________________________________________
activation_3 (Activation)        (None, 32, None, 13)  0           batch_normalization_2[0][0]
____________________________________________________________________________________________________
conv2d_6 (Conv2D)                (None, 32, None, 13)  1056        add_1[0][0]
____________________________________________________________________________________________________
conv2d_5 (Conv2D)                (None, 32, None, 13)  9248        activation_3[0][0]
____________________________________________________________________________________________________
average_pooling2d_4 (AveragePool (None, 32, None, 13)  0           conv2d_6[0][0]
____________________________________________________________________________________________________
average_pooling2d_3 (AveragePool (None, 32, None, 13)  0           conv2d_5[0][0]
____________________________________________________________________________________________________
add_2 (Add)                      (None, 32, None, 13)  0           average_pooling2d_4[0][0]
                                                                   average_pooling2d_3[0][0]
____________________________________________________________________________________________________
batch_normalization_3 (BatchNorm (None, 32, None, 13)  128         add_2[0][0]
____________________________________________________________________________________________________
activation_4 (Activation)        (None, 32, None, 13)  0           batch_normalization_3[0][0]
____________________________________________________________________________________________________
conv2d_7 (Conv2D)                (None, 32, None, 13)  9248        activation_4[0][0]
____________________________________________________________________________________________________
batch_normalization_4 (BatchNorm (None, 32, None, 13)  128         conv2d_7[0][0]
____________________________________________________________________________________________________
activation_5 (Activation)        (None, 32, None, 13)  0           batch_normalization_4[0][0]
____________________________________________________________________________________________________
conv2d_9 (Conv2D)                (None, 32, None, 13)  1056        add_2[0][0]
____________________________________________________________________________________________________
conv2d_8 (Conv2D)                (None, 32, None, 13)  9248        activation_5[0][0]
____________________________________________________________________________________________________
average_pooling2d_6 (AveragePool (None, 32, None, 13)  0           conv2d_9[0][0]
____________________________________________________________________________________________________
average_pooling2d_5 (AveragePool (None, 32, None, 13)  0           conv2d_8[0][0]
____________________________________________________________________________________________________
add_3 (Add)                      (None, 32, None, 13)  0           average_pooling2d_6[0][0]
                                                                   average_pooling2d_5[0][0]
____________________________________________________________________________________________________
batch_normalization_5 (BatchNorm (None, 32, None, 13)  128         add_3[0][0]
____________________________________________________________________________________________________
activation_6 (Activation)        (None, 32, None, 13)  0           batch_normalization_5[0][0]
____________________________________________________________________________________________________
conv2d_10 (Conv2D)               (None, 32, None, 13)  9248        activation_6[0][0]
____________________________________________________________________________________________________
batch_normalization_6 (BatchNorm (None, 32, None, 13)  128         conv2d_10[0][0]
____________________________________________________________________________________________________
activation_7 (Activation)        (None, 32, None, 13)  0           batch_normalization_6[0][0]
____________________________________________________________________________________________________
conv2d_12 (Conv2D)               (None, 32, None, 13)  1056        add_3[0][0]
____________________________________________________________________________________________________
conv2d_11 (Conv2D)               (None, 32, None, 13)  9248        activation_7[0][0]
____________________________________________________________________________________________________
average_pooling2d_8 (AveragePool (None, 32, None, 13)  0           conv2d_12[0][0]
____________________________________________________________________________________________________
average_pooling2d_7 (AveragePool (None, 32, None, 13)  0           conv2d_11[0][0]
____________________________________________________________________________________________________
add_4 (Add)                      (None, 32, None, 13)  0           average_pooling2d_8[0][0]
                                                                   average_pooling2d_7[0][0]
____________________________________________________________________________________________________
batch_normalization_7 (BatchNorm (None, 32, None, 13)  128         add_4[0][0]
____________________________________________________________________________________________________
activation_8 (Activation)        (None, 32, None, 13)  0           batch_normalization_7[0][0]
____________________________________________________________________________________________________
conv2d_13 (Conv2D)               (None, 32, None, 13)  9248        activation_8[0][0]
____________________________________________________________________________________________________
batch_normalization_8 (BatchNorm (None, 32, None, 13)  128         conv2d_13[0][0]
____________________________________________________________________________________________________
activation_9 (Activation)        (None, 32, None, 13)  0           batch_normalization_8[0][0]
____________________________________________________________________________________________________
conv2d_15 (Conv2D)               (None, 32, None, 13)  1056        add_4[0][0]
____________________________________________________________________________________________________
conv2d_14 (Conv2D)               (None, 32, None, 13)  9248        activation_9[0][0]
____________________________________________________________________________________________________
average_pooling2d_10 (AveragePoo (None, 32, None, 13)  0           conv2d_15[0][0]
____________________________________________________________________________________________________
average_pooling2d_9 (AveragePool (None, 32, None, 13)  0           conv2d_14[0][0]
____________________________________________________________________________________________________
add_5 (Add)                      (None, 32, None, 13)  0           average_pooling2d_10[0][0]
                                                                   average_pooling2d_9[0][0]
____________________________________________________________________________________________________
batch_normalization_9 (BatchNorm (None, 32, None, 13)  128         add_5[0][0]
____________________________________________________________________________________________________
activation_10 (Activation)       (None, 32, None, 13)  0           batch_normalization_9[0][0]
____________________________________________________________________________________________________
conv2d_16 (Conv2D)               (None, 32, None, 13)  9248        activation_10[0][0]
____________________________________________________________________________________________________
batch_normalization_10 (BatchNor (None, 32, None, 13)  128         conv2d_16[0][0]
____________________________________________________________________________________________________
activation_11 (Activation)       (None, 32, None, 13)  0           batch_normalization_10[0][0]
____________________________________________________________________________________________________
conv2d_18 (Conv2D)               (None, 32, None, 13)  1056        add_5[0][0]
____________________________________________________________________________________________________
conv2d_17 (Conv2D)               (None, 32, None, 13)  9248        activation_11[0][0]
____________________________________________________________________________________________________
average_pooling2d_12 (AveragePoo (None, 32, None, 13)  0           conv2d_18[0][0]
____________________________________________________________________________________________________
average_pooling2d_11 (AveragePoo (None, 32, None, 13)  0           conv2d_17[0][0]
____________________________________________________________________________________________________
add_6 (Add)                      (None, 32, None, 13)  0           average_pooling2d_12[0][0]
                                                                   average_pooling2d_11[0][0]
____________________________________________________________________________________________________
batch_normalization_11 (BatchNor (None, 32, None, 13)  128         add_6[0][0]
____________________________________________________________________________________________________
activation_12 (Activation)       (None, 32, None, 13)  0           batch_normalization_11[0][0]
____________________________________________________________________________________________________
conv2d_19 (Conv2D)               (None, 1, None, 13)   289         activation_12[0][0]
____________________________________________________________________________________________________
lambda_2 (Lambda)                (None, None, 13)      0           conv2d_19[0][0]
____________________________________________________________________________________________________
bidirectional_1 (Bidirectional)  (None, None, 64)      11776       lambda_2[0][0]
____________________________________________________________________________________________________
soft_attention_1 (SoftAttention) (None, 64)            4224        bidirectional_1[0][0]
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 32)            2080        soft_attention_1[0][0]
____________________________________________________________________________________________________
batch_normalization_12 (BatchNor (None, 32)            128         dense_1[0][0]
____________________________________________________________________________________________________
activation_13 (Activation)       (None, 32)            0           batch_normalization_12[0][0]
____________________________________________________________________________________________________
dropout_1 (Dropout)              (None, 32)            0           activation_13[0][0]
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 32)            1056        dropout_1[0][0]
____________________________________________________________________________________________________
batch_normalization_13 (BatchNor (None, 32)            128         dense_2[0][0]
____________________________________________________________________________________________________
activation_14 (Activation)       (None, 32)            0           batch_normalization_13[0][0]
____________________________________________________________________________________________________
input_2 (InputLayer)             (None, 1)             0
____________________________________________________________________________________________________
input_3 (InputLayer)             (None, 1)             0
____________________________________________________________________________________________________
dropout_2 (Dropout)              (None, 32)            0           activation_14[0][0]
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 34)            0           input_2[0][0]
                                                                   input_3[0][0]
                                                                   dropout_2[0][0]
____________________________________________________________________________________________________
discriminator_output (Dense)     (None, 1)             35          concatenate_1[0][0]
====================================================================================================
Total params: 128,516
Trainable params: 127,684
Non-trainable params: 832
____________________________________________________________________________________________________
Epoch 1/100
2017-09-30 19:51:27.687642: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-09-30 19:51:27.687680: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-09-30 19:51:27.687691: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-09-30 19:51:27.687700: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-09-30 19:51:27.687713: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-09-30 19:51:28.409761: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:
name: GeForce GTX 970
major: 5 minor: 2 memoryClockRate (GHz) 1.253
pciBusID 0000:01:00.0
Total memory: 3.94GiB
Free memory: 3.88GiB
2017-09-30 19:51:28.409784: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0
2017-09-30 19:51:28.409788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y
2017-09-30 19:51:28.409793: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 970, pci bus id: 0000:01:00.0)
 16/588 [..............................] - ETA: 607s - loss: 1.1274 - acc: 0.4141
 58/588 [=>............................] - ETA: 452s - loss: 0.9885 - acc: 0.4461
587/588 [============================>.] - ETA: 0s - loss: 0.7343 - acc: 0.5963
588/588 [==============================] - 690s - loss: 0.7344 - acc: 0.5963 - val_loss: 0.6221 - val_acc: 0.6967
Epoch 2/100
588/588 [==============================] - 664s - loss: 0.6385 - acc: 0.6718 - val_loss: 0.5718 - val_acc: 0.7016
Epoch 3/100
588/588 [==============================] - 663s - loss: 0.5946 - acc: 0.7049 - val_loss: 0.5584 - val_acc: 0.7041
Epoch 4/100
588/588 [==============================] - 655s - loss: 0.5738 - acc: 0.7090 - val_loss: 0.5466 - val_acc: 0.7018
Epoch 5/100
588/588 [==============================] - 655s - loss: 0.5572 - acc: 0.7102 - val_loss: 0.5296 - val_acc: 0.7005
Epoch 6/100
588/588 [==============================] - 652s - loss: 0.5531 - acc: 0.7066 - val_loss: 0.5153 - val_acc: 0.6995
Epoch 7/100
588/588 [==============================] - 652s - loss: 0.5423 - acc: 0.7200 - val_loss: 0.5084 - val_acc: 0.7066
Epoch 8/100
588/588 [==============================] - 652s - loss: 0.5399 - acc: 0.7264 - val_loss: 0.4987 - val_acc: 0.7151
Epoch 9/100
588/588 [==============================] - 649s - loss: 0.5368 - acc: 0.7253 - val_loss: 0.4885 - val_acc: 0.7499
Epoch 10/100
588/588 [==============================] - 650s - loss: 0.5224 - acc: 0.7430 - val_loss: 0.4780 - val_acc: 0.7450
Epoch 11/100
588/588 [==============================] - 650s - loss: 0.5137 - acc: 0.7434 - val_loss: 0.4644 - val_acc: 0.7586
Epoch 12/100
588/588 [==============================] - 648s - loss: 0.5070 - acc: 0.7570 - val_loss: 0.4843 - val_acc: 0.7551
Epoch 13/100
588/588 [==============================] - 647s - loss: 0.5090 - acc: 0.7562 - val_loss: 0.4668 - val_acc: 0.7486
Epoch 14/100
588/588 [==============================] - 647s - loss: 0.5147 - acc: 0.7568 - val_loss: 0.4610 - val_acc: 0.7613
Epoch 15/100
588/588 [==============================] - 648s - loss: 0.4976 - acc: 0.7713 - val_loss: 0.4515 - val_acc: 0.7858
Epoch 16/100
588/588 [==============================] - 647s - loss: 0.4880 - acc: 0.7730 - val_loss: 0.4686 - val_acc: 0.7467
Epoch 17/100
588/588 [==============================] - 646s - loss: 0.4962 - acc: 0.7700 - val_loss: 0.4610 - val_acc: 0.7747
Epoch 18/100
588/588 [==============================] - 651s - loss: 0.4985 - acc: 0.7776 - val_loss: 0.4524 - val_acc: 0.7688
Epoch 19/100
588/588 [==============================] - 645s - loss: 0.5020 - acc: 0.7789 - val_loss: 0.4512 - val_acc: 0.7638
Epoch 20/100
588/588 [==============================] - 649s - loss: 0.4865 - acc: 0.7876 - val_loss: 0.4489 - val_acc: 0.7732
Epoch 21/100
588/588 [==============================] - 644s - loss: 0.4864 - acc: 0.7881 - val_loss: 0.4466 - val_acc: 0.7910
Epoch 22/100
588/588 [==============================] - 644s - loss: 0.4855 - acc: 0.7902 - val_loss: 0.4456 - val_acc: 0.7998
Epoch 23/100
588/588 [==============================] - 645s - loss: 0.4780 - acc: 0.7942 - val_loss: 0.4390 - val_acc: 0.8111
Epoch 24/100
588/588 [==============================] - 645s - loss: 0.4759 - acc: 0.7951 - val_loss: 0.4296 - val_acc: 0.8099
Epoch 25/100
588/588 [==============================] - 650s - loss: 0.4804 - acc: 0.7985 - val_loss: 0.4388 - val_acc: 0.8006
Epoch 26/100
588/588 [==============================] - 646s - loss: 0.4739 - acc: 0.8002 - val_loss: 0.4395 - val_acc: 0.7993
Epoch 27/100
588/588 [==============================] - 647s - loss: 0.4755 - acc: 0.8031 - val_loss: 0.4310 - val_acc: 0.8050
Epoch 28/100
588/588 [==============================] - 645s - loss: 0.4809 - acc: 0.7989 - val_loss: 0.4309 - val_acc: 0.8032
Epoch 29/100
588/588 [==============================] - 646s - loss: 0.4634 - acc: 0.8091 - val_loss: 0.4294 - val_acc: 0.8100
Epoch 30/100
588/588 [==============================] - 648s - loss: 0.4687 - acc: 0.8053 - val_loss: 0.4316 - val_acc: 0.8266
Epoch 31/100
588/588 [==============================] - 646s - loss: 0.4640 - acc: 0.8059 - val_loss: 0.4180 - val_acc: 0.8236
Epoch 32/100
588/588 [==============================] - 647s - loss: 0.4782 - acc: 0.7963 - val_loss: 0.4309 - val_acc: 0.8095
Epoch 33/100
588/588 [==============================] - 647s - loss: 0.4567 - acc: 0.8227 - val_loss: 0.4459 - val_acc: 0.7914
Epoch 34/100
588/588 [==============================] - 645s - loss: 0.4563 - acc: 0.8204 - val_loss: 0.4142 - val_acc: 0.8219
Epoch 35/100
588/588 [==============================] - 643s - loss: 0.4648 - acc: 0.8138 - val_loss: 0.4192 - val_acc: 0.8268
Epoch 36/100
588/588 [==============================] - 644s - loss: 0.4602 - acc: 0.8176 - val_loss: 0.4175 - val_acc: 0.8238
Epoch 37/100
588/588 [==============================] - 645s - loss: 0.4612 - acc: 0.8202 - val_loss: 0.4150 - val_acc: 0.8348
Epoch 38/100
588/588 [==============================] - 643s - loss: 0.4646 - acc: 0.8099 - val_loss: 0.4082 - val_acc: 0.8397
Epoch 39/100
588/588 [==============================] - 645s - loss: 0.4564 - acc: 0.8202 - val_loss: 0.4049 - val_acc: 0.8324
Epoch 40/100
588/588 [==============================] - 643s - loss: 0.4373 - acc: 0.8289 - val_loss: 0.4037 - val_acc: 0.8367
Epoch 41/100
588/588 [==============================] - 644s - loss: 0.4613 - acc: 0.8153 - val_loss: 0.4108 - val_acc: 0.8292
Epoch 42/100
588/588 [==============================] - 645s - loss: 0.4631 - acc: 0.8131 - val_loss: 0.4024 - val_acc: 0.8405
Epoch 43/100
588/588 [==============================] - 642s - loss: 0.4374 - acc: 0.8355 - val_loss: 0.4051 - val_acc: 0.8361
Epoch 44/100
588/588 [==============================] - 648s - loss: 0.4397 - acc: 0.8253 - val_loss: 0.4050 - val_acc: 0.8321
Epoch 45/100
588/588 [==============================] - 647s - loss: 0.4436 - acc: 0.8225 - val_loss: 0.4017 - val_acc: 0.8428
Epoch 46/100
588/588 [==============================] - 648s - loss: 0.4393 - acc: 0.8238 - val_loss: 0.4059 - val_acc: 0.8449
Epoch 47/100
588/588 [==============================] - 646s - loss: 0.4476 - acc: 0.8212 - val_loss: 0.4259 - val_acc: 0.8131
Epoch 48/100
588/588 [==============================] - 644s - loss: 0.4426 - acc: 0.8227 - val_loss: 0.4046 - val_acc: 0.8375
Epoch 49/100
588/588 [==============================] - 644s - loss: 0.4425 - acc: 0.8272 - val_loss: 0.3980 - val_acc: 0.8422
Epoch 50/100
588/588 [==============================] - 640s - loss: 0.4390 - acc: 0.8306 - val_loss: 0.4002 - val_acc: 0.8426
Epoch 51/100
588/588 [==============================] - 646s - loss: 0.4400 - acc: 0.8238 - val_loss: 0.4048 - val_acc: 0.8329
Epoch 52/100
588/588 [==============================] - 645s - loss: 0.4396 - acc: 0.8248 - val_loss: 0.3908 - val_acc: 0.8473
Epoch 53/100
588/588 [==============================] - 642s - loss: 0.4445 - acc: 0.8227 - val_loss: 0.4036 - val_acc: 0.8340
Epoch 54/100
588/588 [==============================] - 644s - loss: 0.4347 - acc: 0.8291 - val_loss: 0.3869 - val_acc: 0.8591
Epoch 55/100
588/588 [==============================] - 646s - loss: 0.4441 - acc: 0.8206 - val_loss: 0.3920 - val_acc: 0.8530
Epoch 56/100
588/588 [==============================] - 644s - loss: 0.4477 - acc: 0.8163 - val_loss: 0.4141 - val_acc: 0.8340
Epoch 57/100
588/588 [==============================] - 648s - loss: 0.4291 - acc: 0.8255 - val_loss: 0.3998 - val_acc: 0.8332
Epoch 58/100
588/588 [==============================] - 642s - loss: 0.4349 - acc: 0.8225 - val_loss: 0.4066 - val_acc: 0.8682
Epoch 59/100
588/588 [==============================] - 646s - loss: 0.4329 - acc: 0.8280 - val_loss: 0.3993 - val_acc: 0.8312
Epoch 60/100
588/588 [==============================] - 643s - loss: 0.4302 - acc: 0.8284 - val_loss: 0.4055 - val_acc: 0.8479
Epoch 61/100
588/588 [==============================] - 643s - loss: 0.4226 - acc: 0.8329 - val_loss: 0.3894 - val_acc: 0.8412
Epoch 62/100
588/588 [==============================] - 644s - loss: 0.4116 - acc: 0.8414 - val_loss: 0.3958 - val_acc: 0.8376

